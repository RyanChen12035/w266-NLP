{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2j1ulzDGpkQD6DZ8/JMn9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyanChen12035/w266-NLP/blob/main/w266_final_model1_prunning_strategy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTz1tADXGQ1f"
      },
      "outputs": [],
      "source": [
        "!pip install pydot --quiet\n",
        "!pip install tensorflow-datasets --quiet\n",
        "!pip install transformers --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "id": "NB6VdlPuHKKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "metadata": {
        "id": "gFSOQbJVHKOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Embedding, Input, Dense, Lambda, Dropout, Conv1D, GlobalMaxPooling1D, Concatenate, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow_datasets as tfds\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from transformers import logging\n",
        "logging.set_verbosity_error()\n",
        "import sklearn as sk\n",
        "import os\n",
        "from nltk.data import find\n",
        "import matplotlib.pyplot as plt\n",
        "import re"
      ],
      "metadata": {
        "id": "OFFic2zrHKPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = tfds.load(\n",
        "    name=\"imdb_reviews\",\n",
        "    split=('train[:80%]', 'test[80%:]'),\n",
        "    as_supervised=True)\n",
        "\n",
        "train_examples, train_labels = next(iter(train_data.batch(20000)))\n",
        "val_examples, val_labels = next(iter(test_data.batch(5000)))\n",
        "test_examples, test_labels = next(iter(test_data.batch(1000)))"
      ],
      "metadata": {
        "id": "n3SFheIgHKQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#allow us to get the hidden layer\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-cased', output_hidden_states=True)\n",
        "MAX_SEQUENCE_LENGTH = 100"
      ],
      "metadata": {
        "id": "t57kt-ygHKSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BERT Tokenization of training and test data\n",
        "#Embedding size of Bert tokenizer: 768\n",
        "#Dictionary size of Bert tokenizer: 28,996\n",
        "\n",
        "\n",
        "train_examples_str = [x.decode('utf-8') for x in train_examples.numpy()]\n",
        "val_examples_str = [x.decode('utf-8') for x in val_examples.numpy()]\n",
        "test_examples_str = [x.decode('utf-8') for x in test_examples.numpy()]\n",
        "\n",
        "#train\n",
        "bert_train_tokenized = bert_tokenizer(train_examples_str,\n",
        "              max_length=MAX_SEQUENCE_LENGTH,\n",
        "              truncation=True,\n",
        "              padding='max_length',\n",
        "              return_tensors='tf')\n",
        "bert_train_inputs = [bert_train_tokenized.input_ids,\n",
        "                     bert_train_tokenized.token_type_ids,\n",
        "                     bert_train_tokenized.attention_mask]\n",
        "bert_train_labels = np.array(train_labels)\n",
        "\n",
        "#val\n",
        "bert_val_tokenized = bert_tokenizer(val_examples_str,\n",
        "              max_length=MAX_SEQUENCE_LENGTH,\n",
        "              truncation=True,\n",
        "              padding='max_length',\n",
        "              return_tensors='tf')\n",
        "bert_val_inputs = [bert_val_tokenized.input_ids,\n",
        "                     bert_val_tokenized.token_type_ids,\n",
        "                     bert_val_tokenized.attention_mask]\n",
        "bert_val_labels = np.array(val_labels)\n",
        "\n",
        "\n",
        "#test\n",
        "bert_test_tokenized = bert_tokenizer(test_examples_str,\n",
        "              max_length=MAX_SEQUENCE_LENGTH,\n",
        "              truncation=True,\n",
        "              padding='max_length',\n",
        "              return_tensors='tf')\n",
        "bert_test_inputs = [bert_test_tokenized.input_ids,\n",
        "                     bert_test_tokenized.token_type_ids,\n",
        "                     bert_test_tokenized.attention_mask]\n",
        "bert_test_labels = np.array(test_labels)"
      ],
      "metadata": {
        "id": "WyudtBAlHKUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#12 layers of transformer\n",
        "#A drop out layer + dense layer with 100 hidden layer size on top + final layer with sigmoid as activation function\n",
        "\n",
        "def create_bert_cls_model(bert_base_model,\n",
        "                          max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "                          hidden_size = 100,\n",
        "                          dropout=0.3,\n",
        "                          learning_rate=0.00005,\n",
        "                          output_cls_tokens=False):\n",
        "    \"\"\"\n",
        "    Build a simple classification model with BERT. Use the CLS Token output for classification purposes.\n",
        "    \"\"\"\n",
        "\n",
        "    bert_base_model.trainable = True #True\n",
        "\n",
        "    #input layers of BERT, shape (batch, max_sequence_length), model will be fit with bert_train_tokenized\n",
        "    input_ids = Input(shape=(max_sequence_length,), dtype=tf.int32, name='input_ids')\n",
        "    token_type_ids = Input(shape=(max_sequence_length,), dtype=tf.int32, name='token_type_ids')\n",
        "    attention_mask = Input(shape=(max_sequence_length,), dtype=tf.int32, name='attention_mask')\n",
        "\n",
        "    inputs = [input_ids, token_type_ids, attention_mask]\n",
        "\n",
        "    #BERT output, last_hidden_state shape (batch, max_sequence_length, embedding dimensions)\n",
        "    bert_output = bert_base_model(input_ids=input_ids,\n",
        "                                  token_type_ids=token_type_ids,\n",
        "                                  attention_mask=attention_mask,\n",
        "                                  output_hidden_states=output_cls_tokens)\n",
        "\n",
        "    #Extract the CLS token's output, the embedding representation of first token of every sentence, shape(batch, embedding dimensions)\n",
        "    cls_token_output = bert_output[0][:, 0, :] # CLS token output from the last layer\n",
        "\n",
        "    #Add a dropout layer\n",
        "    x = Dropout(dropout)(cls_token_output)\n",
        "\n",
        "    #Add a fully connected layer for classification\n",
        "    x = Dense(hidden_size, activation='relu')(x)\n",
        "\n",
        "    #Final output layer for classification, assuming it's binary task\n",
        "    output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "\n",
        "    # CLS output for each layer of transformer\n",
        "    if output_cls_tokens:\n",
        "        cls_outputs = [state[:, 0, :] for state in bert_output[2]] # CLS token outputs from all layers\n",
        "        model_outputs = [output] + cls_outputs\n",
        "\n",
        "    else:\n",
        "        model_outputs = output\n",
        "\n",
        "\n",
        "    #Model complie\n",
        "    classification_model = Model(inputs=inputs, outputs=model_outputs)\n",
        "    classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                                 loss='binary_crossentropy',\n",
        "                                 metrics=['accuracy'])\n",
        "\n",
        "    return classification_model\n",
        "\n",
        "\"\"\"\n",
        "bert_output[2]: When the output_hidden_states parameter is set to True, this output provides the hidden states from all layers of the BERT model.\n",
        "It is a list of tensors, where each tensor corresponds to the hidden states of a specific layer.\n",
        "The shape of each tensor in this list is (batch_size, sequence_length, hidden_size), similar to bert_output[0], but for each individual layer.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "jSzxLJHQHKXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "qkVCh_CJHTny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bert_model\n",
        "bert_cls_model_classification = create_bert_cls_model(bert_model, output_cls_tokens=False)\n",
        "history_cls_bert= bert_cls_model_classification.fit(bert_train_inputs,\n",
        "                                                    bert_train_labels,\n",
        "                                                    epochs=2, #2\n",
        "                                                    batch_size=8, #8\n",
        "                                                    validation_data=(bert_val_inputs, bert_val_labels))"
      ],
      "metadata": {
        "id": "Z86T04iGHTqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model before zeroing out\n",
        "\n",
        "bert_cls_model_classification.evaluate(bert_test_inputs, bert_test_labels)"
      ],
      "metadata": {
        "id": "W_vZhrtWHTtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "prediction = bert_cls_model_classification.predict(bert_test_inputs)\n",
        "end_time = time.time()\n",
        "\n",
        "elapsed_time = end_time - start_time\n",
        "print(\"Elapsed time: {:.2f} seconds\".format(elapsed_time))"
      ],
      "metadata": {
        "id": "yfmDk4RkHYP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example test reviews\n",
        "\"\"\"\n",
        "1. Identifying Emotional Tone\n",
        "Sub-Task: Determine the emotional tone of the review (e.g., positive, negative, neutral).\n",
        "Test Reviews:\n",
        "    EX1 \"The movie's breathtaking scenery and exceptional soundtrack added depth to its rich storytelling.\" -> Positive Tone\n",
        "    EX2 \"The film was a letdown with its lackluster plot and uninspired performances.\" -> Negative Tone\n",
        "2. Analyzing Subjective Statements\n",
        "Sub-Task: Detect subjective statements or opinions in the review.\n",
        "Test Reviews:\n",
        "    EX3 \"In my opinion, the film's portrayal of historical events was highly inaccurate.\" -> Subjective\n",
        "    EX4 \"The movie won three Academy Awards, including Best Picture.\" -> Objective\n",
        "3. Evaluating Specific Aspects (Acting, Plot, Cinematography)\n",
        "Sub-Task: Assess specific aspects of the movie like acting quality, plot development, and cinematography.\n",
        "Test Reviews:\n",
        "    EX5 \"The acting was superb, with each character bringing depth and emotion to the screen.\" -> Positive Acting\n",
        "    EX6 \"The plot was predictable and lacked originality, making the movie quite boring.\" -> Negative Plot\n",
        "4. Recognizing Extremes in Sentiment\n",
        "Sub-Task: Identify reviews with extremely positive or negative sentiments.\n",
        "Test Reviews:\n",
        "    EX7 \"This is possibly the worst movie ever made, with no redeeming qualities whatsoever.\" -> Extremely Negative\n",
        "    EX8 \"An absolute masterpiece, every moment was captivating and a joy to watch.\" -> Extremely Positive\n",
        "5. Detecting Sarcasm or Irony\n",
        "Sub-Task: Detect sarcasm or irony, which can often invert the apparent sentiment of a statement.\n",
        "Test Reviews:\n",
        "    EX9 \"Oh great, another predictable rom-com, just what the world needs.\" -> Sarcasm\n",
        "    EX10 \"I loved how the movie ended abruptly without resolving any plot points.\" -> Irony\n",
        "\"\"\"\n",
        "\n",
        "# First reivews is positive tone and the second is negative tone\n",
        "test_reviews = [\n",
        "    \"In my opinion, the film's portrayal of historical events was highly inaccurate.\",\n",
        "    \"The movie won three Academy Awards, including Best Picture.\",\n",
        "    \"In my opinion, the film's portrayal of historical events was highly inaccurate.\",\n",
        "    \"The movie won three Academy Awards, including Best Picture.\",\n",
        "    \"The acting was superb, with each character bringing depth and emotion to the screen.\",\n",
        "    \"The plot was predictable and lacked originality, making the movie quite boring.\",\n",
        "    \"This is possibly the worst movie ever made, with no redeeming qualities whatsoever.\",\n",
        "    \"An absolute masterpiece, every moment was captivating and a joy to watch.\",\n",
        "    \"Oh great, another predictable rom-com, just what the world needs.\",\n",
        "    \"I loved how the movie ended abruptly without resolving any plot points.\"\n",
        "]\n",
        "\n",
        "# Tokenize the reviews\n",
        "# 101:[CLS], 102:[SEP]\n",
        "token_inputs = bert_tokenizer(test_reviews,\n",
        "                            max_length=MAX_SEQUENCE_LENGTH,\n",
        "                            truncation=True,\n",
        "                            padding='max_length',\n",
        "                            return_tensors='tf')\n",
        "\n",
        "inputs = [token_inputs.input_ids,\n",
        "        token_inputs.token_type_ids,\n",
        "        token_inputs.attention_mask]\n",
        "\n",
        "print(inputs)"
      ],
      "metadata": {
        "id": "lCKi6iRQHYT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a model for analysis which includes the hidden states\n",
        "#bert_model is fine-tuned now, don't have to re-train it. it's an object.\n",
        "cls_layer_inside_finetunedBERT = create_bert_cls_model(bert_model, output_cls_tokens=True)\n",
        "\n",
        "#get cls in each layers of transformer inside the fine-tuned BERT\n",
        "predictions, *cls_hidden_states = cls_layer_inside_finetunedBERT.predict(inputs)\n",
        "print(cls_hidden_states)"
      ],
      "metadata": {
        "id": "r6FMegUzHYVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def calculate_similarities(cls_outputs):\n",
        "    # Number of examples and layers\n",
        "    num_examples, num_layers = cls_outputs[0].shape[0], len(cls_outputs)\n",
        "\n",
        "    # Initializing arrays to store the results\n",
        "    cosine_similarities = np.zeros((num_examples, num_layers))\n",
        "    dot_products = np.zeros((num_examples, num_layers))\n",
        "\n",
        "    # Final layer's CLS output\n",
        "    final_layer_output = cls_outputs[-1]\n",
        "\n",
        "    # Calculating similarities and dot products\n",
        "    for i in range(num_layers):\n",
        "        for j in range(num_examples):\n",
        "            # Extracting the CLS output for the current layer and example\n",
        "            current_output = cls_outputs[i][j]\n",
        "\n",
        "            # Cosine Similarity\n",
        "            cosine_similarities[j, i] = np.dot(current_output, final_layer_output[j]) / (norm(current_output) * norm(final_layer_output[j]))\n",
        "\n",
        "            # Dot Product\n",
        "            dot_products[j, i] = np.dot(current_output, final_layer_output[j])\n",
        "\n",
        "    return cosine_similarities, dot_products\n",
        "\n",
        "\n",
        "cosine_similarities_layer = calculate_similarities(cls_hidden_states)"
      ],
      "metadata": {
        "id": "BGAd-xlCHYXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for var in bert_model.variables:\n",
        "    print(f\"{var.name}: {var.shape}\")"
      ],
      "metadata": {
        "id": "NaoZPXk9Hl5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_ffn_second_dense_weights(bert_model):\n",
        "    \"\"\"\n",
        "    Extracts the kernel weights from the second dense layer of the FFN in each transformer layer of the BERT model.\n",
        "    \"\"\"\n",
        "    ffn_weights = []\n",
        "\n",
        "    # Loop through each transformer layer and construct the variable name\n",
        "    for layer_num in range(bert_model.config.num_hidden_layers):\n",
        "        # Construct the variable name for the second dense layer weights in the current layer\n",
        "        weight_name = f\"tf_bert_model/bert/encoder/layer_._{layer_num}/output/dense/kernel:0\"\n",
        "\n",
        "        # Find and extract the variable\n",
        "        for var in bert_model.variables:\n",
        "            if var.name == weight_name:\n",
        "                weights = var.numpy()  # Convert to numpy array\n",
        "                ffn_weights.append(weights)\n",
        "                break  # Move to the next layer once the weights are found\n",
        "\n",
        "    return ffn_weights\n",
        "\n",
        "ffn_weights = extract_ffn_second_dense_weights(bert_model)"
      ],
      "metadata": {
        "id": "2V-e1tFTHqJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input of EX1. cls_hidden_states (layer, example, embedding dimensions)\n",
        "cls_token_output = cls_hidden_states[12][0][:]\n",
        "\n",
        "def compute_cosine_similarities(cls_token_output, ffn_weights):\n",
        "    # Normalize the CLS token output\n",
        "    cls_norm = np.linalg.norm(cls_token_output)\n",
        "    cls_token_normalized = cls_token_output / cls_norm\n",
        "\n",
        "    cosine_similarities = []\n",
        "\n",
        "    for layer_weights in ffn_weights:\n",
        "        # Transpose the weights to align dimensions with CLS token output\n",
        "        # layer_weights shape is (3072, 768), after transpose it will be (768, 3072)\n",
        "        transposed_weights = layer_weights.T\n",
        "\n",
        "        # Normalize the neuron weights\n",
        "        neuron_norms = np.linalg.norm(transposed_weights, axis=0)\n",
        "        normalized_neurons = transposed_weights / neuron_norms\n",
        "\n",
        "        # Compute the dot product\n",
        "        dot_product = np.dot(cls_token_normalized, normalized_neurons)\n",
        "\n",
        "        cosine_similarities.append(dot_product)\n",
        "\n",
        "    return cosine_similarities\n",
        "\n",
        "#'cls_token_output' is the CLS token output of shape (768,)\n",
        "# And 'ffn_weights' is a list of arrays, each of shape (3072, 768)\n",
        "cosine_similarities_neurons = compute_cosine_similarities(cls_token_output, ffn_weights)\n",
        "\n",
        "print(cosine_similarities_neurons)"
      ],
      "metadata": {
        "id": "kJH1aBnKHqNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#input 5, layer 1, first layer of transfomrers\n",
        "#output: masked_neurons_list\n",
        "#input: cosine_similarities_neurons, cosine_similarities_layer, alpha\n",
        "#if the last token of layer N is close to last layer, it means Nth layer get the right information to make the call and neruons in this layer can handle the task properly,\n",
        "#As a result, neurons having low simiarlarity in this layer can be removed. The bar can be slightly higher\n",
        "#cosine similarity can be negative, cos_sin = 0 irrelavent, -1~0, 0~1\n",
        "\n",
        "preserved_neuron_list = [[75, 239, 283, 307, 489, 537, 2018, 2625, 2670, 2773, 765, 1669],\n",
        "                       [11, 113, 376, 524, 672, 1685, 1766, 1838],\n",
        "                       [57, 789, 923,1316 ,2719, 2801, 3062],\n",
        "                       [323, 693, 1469, 1795, 2233],\n",
        "                       [102, 170, 642, 915, 2116, 2548],\n",
        "                       [55],\n",
        "                       [1443, 1506, 1712],\n",
        "                       [],\n",
        "                       [763],\n",
        "                       [2528],\n",
        "                       [],\n",
        "                       [1023]]\n",
        "\n",
        "\n",
        "def pruning_strategy(cosine_similarities_neurons, cosine_similarities_layer ,preserved_neuron_list, alpha):\n",
        "    masked_neurons_list = [[] for i in range(12)]\n",
        "\n",
        "    for layer in range(9):\n",
        "        if layer %% 2 == 0: #even layers -> tight\n",
        "            for num, simi in enumerate(cosine_similarities_neurons[layer]):\n",
        "                if 1.2*cosine_similarities_layer[5][layer]*alpha > simi > -1.2*cosine_similarities_layer[5][layer]*alpha:\n",
        "                    masked_neurons_list[layer].append(num)\n",
        "        else: #odd layers -> loose\n",
        "            for num, simi in enumerate(cosine_similarities_neurons[layer]):\n",
        "                if 0.8*cosine_similarities_layer[5][layer]*alpha > simi > -0.8*cosine_similarities_layer[5][layer]*alpha:\n",
        "                    masked_neurons_list[layer].append(num)\n",
        "        print(f\"Threshold of pruning {cosine_similarities_layer[5][layer]*alpha}\")\n",
        "        print(f\"number of neruons being masked in layer {layer}: {len(masked_neurons_list[layer])}\")\n",
        "\n",
        "    #layer 9,10,11 -> very tight\n",
        "    for layer in range(9,12):\n",
        "        for num, simi in enumerate(cosine_similarities_neurons[layer]):\n",
        "            if simi < 3*cosine_similarities_layer[5][layer]*alpha:\n",
        "                masked_neurons_list[layer].append(num)\n",
        "        print(f\"Threshold of pruning {cosine_similarities_layer[5][layer]*alpha}\")\n",
        "        print(f\"number of neruons being masked in layer {layer}: {len(masked_neurons_list[layer])}\")\n",
        "\n",
        "    return masked_neurons_list\n",
        "\n",
        "masked_neurons_list = pruning_strategy(cosine_similarities_neurons, cosine_similarities_layer,preserved_neuron_list, alpha=0.1)\n",
        "print(masked_neurons_list)"
      ],
      "metadata": {
        "id": "UhmYbX1OHqWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#zero out the weights except presrved neurons.\n",
        "preserved_neuron_list = [[75, 239, 283, 307, 489, 537, 2018, 2625, 2670, 2773, 765, 1669],\n",
        "                       [11, 113, 376, 524, 672, 1685, 1766, 1838],\n",
        "                       [57, 789, 923,1316 ,2719, 2801, 3062],\n",
        "                       [323, 693, 1469, 1795, 2233],\n",
        "                       [102, 170, 642, 915, 2116, 2548],\n",
        "                       [55],\n",
        "                       [1443, 1506, 1712],\n",
        "                       [],\n",
        "                       [763],\n",
        "                       [2528],\n",
        "                       [],\n",
        "                       [1023]]\n",
        "\n",
        "num_neurons = 3072\n",
        "masks = []\n",
        "\n",
        "\n",
        "for i, masked_neurons in enumerate(masked_neurons_list):\n",
        "    mask = np.ones(num_neurons)\n",
        "    if masked_neurons not in preserved_neuron_list[i]:\n",
        "      mask[masked_neurons] = 0\n",
        "      masks.append(mask)"
      ],
      "metadata": {
        "id": "BUjZg5egHqY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#zero out -> quantization + sparse matrix pruning\n",
        "# Assuming bert_model is your pre-trained BERT model\n",
        "#tf_bert_model/bert/encoder/layer_._0/output/dense/kernel\n",
        "\n",
        "for var in bert_model.variables:\n",
        "    if 'output/dense/kernel' in var.name and 'attention' not in var.name:\n",
        "        # Extract layer number from variable name\n",
        "        layer_num = int(var.name.split('/')[3].split('_')[2])\n",
        "\n",
        "        # Get the current weights\n",
        "        weights = var.numpy()\n",
        "\n",
        "        # Apply the mask #(3072,)\n",
        "        mask = masks[layer_num]\n",
        "        weights *= mask.reshape(-1, 1)  # Reshape mask and apply to weights\n",
        "\n",
        "        # Assign the modified weights back to the variable\n",
        "        var.assign(weights)\n"
      ],
      "metadata": {
        "id": "RlvVtK5SHqbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bert_model has been zeroed out.\n",
        "bert_cls_model_classification.evaluate(bert_test_inputs, bert_test_labels)\n",
        "\n",
        "#            accuracy process time (1000 inputs)\n",
        "\n",
        "\n",
        "\n",
        "#Impending for further verifying"
      ],
      "metadata": {
        "id": "83uU4KlpIJu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "prediction = bert_cls_model_classification.predict(bert_test_inputs)\n",
        "end_time = time.time()\n",
        "\n",
        "elapsed_time = end_time - start_time\n",
        "print(\"Elapsed time: {:.2f} seconds\".format(elapsed_time))"
      ],
      "metadata": {
        "id": "6FfQT-IqIJzy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}