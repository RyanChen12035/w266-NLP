{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyanChen12035/w266-NLP/blob/main/w266_final_project_mode1_pruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YaFKyXspi5kp"
      },
      "outputs": [],
      "source": [
        "!pip install pydot --quiet\n",
        "!pip install tensorflow-datasets --quiet\n",
        "!pip install transformers --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "id": "pgXwMWO2i6dO",
        "outputId": "064baceb-ada7-4f8f-a32a-44eb10af0a2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "metadata": {
        "id": "UzfrZYRMi7rY",
        "outputId": "4f310db2-5f74-4528-9567-70461ff05a97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-f5a96a17-f8fb-9640-302b-e8fb1792910a)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Embedding, Input, Dense, Lambda, Dropout, Conv1D, GlobalMaxPooling1D, Concatenate, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow_datasets as tfds\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from transformers import logging\n",
        "logging.set_verbosity_error()\n",
        "import sklearn as sk\n",
        "import os\n",
        "from nltk.data import find\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from tensorflow.keras.utils import custom_object_scope"
      ],
      "metadata": {
        "id": "yPa__T8ci8ga"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = tfds.load(\n",
        "    name=\"imdb_reviews\",\n",
        "    split=('train[:80%]', 'test[80%:]'),\n",
        "    as_supervised=True)\n",
        "\n",
        "train_examples, train_labels = next(iter(train_data.batch(20000)))\n",
        "val_examples, val_labels = next(iter(test_data.batch(5000)))\n",
        "test_examples, test_labels = next(iter(test_data.batch(1000)))"
      ],
      "metadata": {
        "id": "MQRwueBNi9dY"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#allow us to get the hidden layer\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-cased', output_hidden_states=True)\n",
        "MAX_SEQUENCE_LENGTH = 100"
      ],
      "metadata": {
        "id": "bGsFfdYZjBal"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BERT Tokenization of training and test data\n",
        "#Embedding size of Bert tokenizer: 768\n",
        "#Dictionary size of Bert tokenizer: 28,996\n",
        "\n",
        "\n",
        "train_examples_str = [x.decode('utf-8') for x in train_examples.numpy()]\n",
        "val_examples_str = [x.decode('utf-8') for x in val_examples.numpy()]\n",
        "test_examples_str = [x.decode('utf-8') for x in test_examples.numpy()]\n",
        "\n",
        "#train\n",
        "bert_train_tokenized = bert_tokenizer(train_examples_str,\n",
        "              max_length=MAX_SEQUENCE_LENGTH,\n",
        "              truncation=True,\n",
        "              padding='max_length',\n",
        "              return_tensors='tf')\n",
        "bert_train_inputs = [bert_train_tokenized.input_ids,\n",
        "                     bert_train_tokenized.token_type_ids,\n",
        "                     bert_train_tokenized.attention_mask]\n",
        "bert_train_labels = np.array(train_labels)\n",
        "\n",
        "#val\n",
        "bert_val_tokenized = bert_tokenizer(val_examples_str,\n",
        "              max_length=MAX_SEQUENCE_LENGTH,\n",
        "              truncation=True,\n",
        "              padding='max_length',\n",
        "              return_tensors='tf')\n",
        "bert_val_inputs = [bert_val_tokenized.input_ids,\n",
        "                     bert_val_tokenized.token_type_ids,\n",
        "                     bert_val_tokenized.attention_mask]\n",
        "bert_val_labels = np.array(val_labels)\n",
        "\n",
        "\n",
        "#test\n",
        "bert_test_tokenized = bert_tokenizer(test_examples_str,\n",
        "              max_length=MAX_SEQUENCE_LENGTH,\n",
        "              truncation=True,\n",
        "              padding='max_length',\n",
        "              return_tensors='tf')\n",
        "bert_test_inputs = [bert_test_tokenized.input_ids,\n",
        "                     bert_test_tokenized.token_type_ids,\n",
        "                     bert_test_tokenized.attention_mask]\n",
        "bert_test_labels = np.array(test_labels)"
      ],
      "metadata": {
        "id": "YWI5pXdqjF5a"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#12 layers of transformer\n",
        "#A drop out layer + dense layer with 100 hidden layer size on top + final layer with sigmoid as activation function\n",
        "\n",
        "def create_bert_cls_model(bert_base_model,\n",
        "                          max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "                          hidden_size = 100,\n",
        "                          dropout=0.3,\n",
        "                          learning_rate=0.00005,\n",
        "                          output_cls_tokens=False):\n",
        "    \"\"\"\n",
        "    Build a simple classification model with BERT. Use the CLS Token output for classification purposes.\n",
        "    \"\"\"\n",
        "\n",
        "    bert_base_model.trainable = True #True\n",
        "\n",
        "    #input layers of BERT, shape (batch, max_sequence_length), model will be fit with bert_train_tokenized\n",
        "    input_ids = Input(shape=(max_sequence_length,), dtype=tf.int32, name='input_ids')\n",
        "    token_type_ids = Input(shape=(max_sequence_length,), dtype=tf.int32, name='token_type_ids')\n",
        "    attention_mask = Input(shape=(max_sequence_length,), dtype=tf.int32, name='attention_mask')\n",
        "\n",
        "    inputs = [input_ids, token_type_ids, attention_mask]\n",
        "\n",
        "    #BERT output, last_hidden_state shape (batch, max_sequence_length, embedding dimensions)\n",
        "    bert_output = bert_base_model(input_ids=input_ids,\n",
        "                                  token_type_ids=token_type_ids,\n",
        "                                  attention_mask=attention_mask,\n",
        "                                  output_hidden_states=output_cls_tokens)\n",
        "\n",
        "    #Extract the CLS token's output, the embedding representation of first token of every sentence, shape(batch, embedding dimensions)\n",
        "    cls_token_output = bert_output[0][:, 0, :] # CLS token output from the last layer\n",
        "\n",
        "    #Add a dropout layer\n",
        "    x = Dropout(dropout)(cls_token_output)\n",
        "\n",
        "    #Add a fully connected layer for classification\n",
        "    x = Dense(hidden_size, activation='relu')(x)\n",
        "\n",
        "    #Final output layer for classification, assuming it's binary task\n",
        "    output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "\n",
        "    # CLS output for each layer of transformer\n",
        "    if output_cls_tokens:\n",
        "        cls_outputs = [state[:, 0, :] for state in bert_output[2]] # CLS token outputs from all layers\n",
        "        model_outputs = [output] + cls_outputs\n",
        "\n",
        "    else:\n",
        "        model_outputs = output\n",
        "\n",
        "\n",
        "    #Model complie\n",
        "    classification_model = Model(inputs=inputs, outputs=model_outputs)\n",
        "    classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                                 loss='binary_crossentropy',\n",
        "                                 metrics=['accuracy'])\n",
        "\n",
        "    return classification_model\n",
        "\n",
        "\"\"\"\n",
        "bert_output[2]: When the output_hidden_states parameter is set to True, this output provides the hidden states from all layers of the BERT model.\n",
        "It is a list of tensors, where each tensor corresponds to the hidden states of a specific layer.\n",
        "The shape of each tensor in this list is (batch_size, sequence_length, hidden_size), similar to bert_output[0], but for each individual layer.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "mMYL0NuDjHVd",
        "outputId": "9f891cee-56c1-49bc-96fc-adafe6602b94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nbert_output[2]: When the output_hidden_states parameter is set to True, this output provides the hidden states from all layers of the BERT model.\\nIt is a list of tensors, where each tensor corresponds to the hidden states of a specific layer.\\nThe shape of each tensor in this list is (batch_size, sequence_length, hidden_size), similar to bert_output[0], but for each individual layer.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "y9xtOOZvjI_3"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bert_model\n",
        "bert_cls_model_classification = create_bert_cls_model(bert_model, output_cls_tokens=False)\n",
        "history_cls_bert= bert_cls_model_classification.fit(bert_train_inputs,\n",
        "                                                    bert_train_labels,\n",
        "                                                    epochs=2, #2\n",
        "                                                    batch_size=8, #8\n",
        "                                                    validation_data=(bert_val_inputs, bert_val_labels))"
      ],
      "metadata": {
        "id": "CFoto5jDjKCd",
        "outputId": "a9a4b215-ac42-49c1-c953-fa392a4828dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2500/2500 [==============================] - 536s 201ms/step - loss: 0.4324 - accuracy: 0.8002 - val_loss: 0.3783 - val_accuracy: 0.8344\n",
            "Epoch 2/2\n",
            "2500/2500 [==============================] - 499s 199ms/step - loss: 0.2887 - accuracy: 0.8803 - val_loss: 0.3664 - val_accuracy: 0.8414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save the model\n",
        "# Assuming 'bert_cls_model_classification' is your trained model\n",
        "model_h5_path = \"content/sample_data/save/model_finetuned_BERT.h5\"  # Replace with your desired path\n",
        "\n",
        "# Register TFBertModel as a custom object\n",
        "with custom_object_scope({'TFBertModel': TFBertModel}):\n",
        "    bert_cls_model_classification.save(model_h5_path)"
      ],
      "metadata": {
        "id": "y2kLS7etjKao",
        "outputId": "a97a255e-a613-4876-e29f-b9d8f1aefaf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model before zeroing out\n",
        "\n",
        "bert_cls_model_classification.evaluate(bert_test_inputs, bert_test_labels)"
      ],
      "metadata": {
        "id": "vCckrv5BWK1y",
        "outputId": "ae1e6722-ae8a-4ace-c860-cf3161ff8239",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 10s 214ms/step - loss: 0.3518 - accuracy: 0.8520\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3517876863479614, 0.8519999980926514]"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #load the model\n",
        "# with custom_object_scope({'TFBertModel': TFBertModel}):\n",
        "#     bert_cls_model_classification = tf.keras.models.load_model(model_h5_path)"
      ],
      "metadata": {
        "id": "HrwgJoWfjMQw"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_list = list(range(3072))\n",
        "np.random.shuffle(original_list)\n",
        "print(original_list[:307])\n",
        "\n",
        "masked_neurons_list = [original_list[:307]]*12\n",
        "len(masked_neurons_list)"
      ],
      "metadata": {
        "id": "CiTDmnSDOTJH",
        "outputId": "84959dba-3f39-4525-c8ca-a0db62416a95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[567, 1912, 1377, 2437, 2362, 3063, 1278, 1296, 2234, 2949, 2397, 1573, 240, 88, 2621, 312, 631, 77, 1540, 750, 2126, 1991, 2693, 543, 136, 128, 1662, 1317, 909, 700, 1568, 597, 2142, 614, 1376, 204, 2741, 1390, 2975, 1727, 2707, 503, 415, 235, 187, 3, 1042, 2623, 728, 2668, 346, 2149, 1764, 1240, 954, 1619, 316, 1323, 1330, 1094, 856, 1478, 629, 1640, 2105, 2115, 306, 1587, 1856, 2942, 2720, 2401, 2660, 1947, 310, 2656, 2076, 2569, 655, 1294, 948, 236, 443, 1034, 1694, 1481, 2611, 1719, 1501, 1282, 1236, 1178, 1329, 1071, 2496, 1790, 2783, 1488, 775, 2318, 3070, 1452, 1599, 1672, 1821, 2489, 1314, 1147, 1375, 1995, 2827, 2652, 1074, 124, 1249, 99, 1591, 2493, 968, 1032, 560, 1996, 3000, 297, 2653, 1870, 2470, 157, 843, 2404, 2132, 1223, 893, 2232, 1108, 2202, 2069, 1062, 2182, 2486, 374, 2560, 2873, 168, 2248, 169, 533, 1940, 369, 207, 405, 2319, 477, 647, 1593, 2469, 2218, 1424, 2819, 2348, 740, 1025, 623, 2885, 92, 681, 1492, 2227, 2929, 686, 14, 1383, 1659, 1734, 994, 549, 2415, 2804, 1425, 2264, 2010, 288, 2759, 1682, 1300, 2683, 3064, 865, 1135, 6, 2360, 1093, 2159, 781, 816, 1992, 2250, 2630, 1934, 1158, 2716, 2488, 1304, 266, 1372, 906, 2340, 229, 379, 3015, 216, 2349, 451, 2760, 916, 1504, 622, 2695, 764, 1313, 1198, 1403, 132, 1401, 257, 2762, 2157, 2477, 894, 1445, 361, 2385, 2100, 199, 156, 780, 1751, 110, 2571, 1402, 72, 3008, 2608, 1716, 828, 1707, 2808, 2478, 2542, 2492, 1655, 551, 2320, 1547, 29, 586, 1796, 1652, 2454, 2768, 799, 43, 2734, 2329, 1703, 2943, 1816, 2422, 1849, 2966, 268, 2935, 1185, 698, 1311, 2538, 938, 520, 1286, 753, 322, 332, 1298, 736, 2512, 2204, 2751, 26, 625, 375, 576, 710, 1055, 2479, 330, 691, 1949, 1836, 1361, 2821, 2924, 506, 2590, 214, 2043, 441, 1259]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Cosine similiarity >　0.3\n",
        "\n",
        "=================Input 0====================\n",
        "Layer 0, Neurons: ['Layer 0 Neuron 75', 'Layer 0 Neuron 239', 'Layer 0 Neuron 283', 'Layer 0 Neuron 307', 'Layer 0 Neuron 489', 'Layer 0 Neuron 537', 'Layer 0 Neuron 2018', 'Layer 0 Neuron 2625', 'Layer 0 Neuron 2670', 'Layer 0 Neuron 2773']\n",
        "Layer 1, Neurons: ['Layer 1 Neuron 11', 'Layer 1 Neuron 113', 'Layer 1 Neuron 376', 'Layer 1 Neuron 524', 'Layer 1 Neuron 672', 'Layer 1 Neuron 1685', 'Layer 1 Neuron 1766', 'Layer 1 Neuron 1838']\n",
        "Layer 2, Neurons: ['Layer 2 Neuron 57', 'Layer 2 Neuron 789', 'Layer 2 Neuron 923', 'Layer 2 Neuron 1316', 'Layer 2 Neuron 2719', 'Layer 2 Neuron 2801', 'Layer 2 Neuron 3062']\n",
        "Layer 3, Neurons: ['Layer 3 Neuron 323', 'Layer 3 Neuron 693', 'Layer 3 Neuron 1469', 'Layer 3 Neuron 1795', 'Layer 3 Neuron 2233']\n",
        "Layer 4, Neurons: ['Layer 4 Neuron 102', 'Layer 4 Neuron 170', 'Layer 4 Neuron 642', 'Layer 4 Neuron 915', 'Layer 4 Neuron 2116', 'Layer 4 Neuron 2548']\n",
        "Layer 5, Neurons: ['Layer 5 Neuron 55']\n",
        "Layer 6, Neurons: ['Layer 6 Neuron 1443', 'Layer 6 Neuron 1506', 'Layer 6 Neuron 1712']\n",
        "Layer 7, Neurons: []\n",
        "Layer 8, Neurons: ['Layer 8 Neuron 763']\n",
        "Layer 9, Neurons: ['Layer 9 Neuron 2528']\n",
        "Layer 10, Neurons: []\n",
        "Layer 11, Neurons: ['Layer 11 Neuron 1023']\n",
        "\n",
        "=================Input 1====================\n",
        "Layer 0, Neurons: ['Layer 0 Neuron 75', 'Layer 0 Neuron 283', 'Layer 0 Neuron 489', 'Layer 0 Neuron 2625', 'Layer 0 Neuron 2773']\n",
        "Layer 1, Neurons: ['Layer 1 Neuron 1685', 'Layer 1 Neuron 1766']\n",
        "Layer 2, Neurons: ['Layer 2 Neuron 789']\n",
        "Layer 3, Neurons: ['Layer 3 Neuron 693']\n",
        "Layer 4, Neurons: ['Layer 4 Neuron 170']\n",
        "Layer 5, Neurons: []\n",
        "Layer 6, Neurons: ['Layer 6 Neuron 1443']\n",
        "Layer 7, Neurons: []\n",
        "Layer 8, Neurons: ['Layer 8 Neuron 763']\n",
        "Layer 9, Neurons: ['Layer 9 Neuron 2528']\n",
        "Layer 10, Neurons: []\n",
        "Layer 11, Neurons: []\n",
        "\n",
        "=================Input 2====================\n",
        "Layer 0, Neurons: ['Layer 0 Neuron 75', 'Layer 0 Neuron 239', 'Layer 0 Neuron 283', 'Layer 0 Neuron 307', 'Layer 0 Neuron 489', 'Layer 0 Neuron 537', 'Layer 0 Neuron 2018', 'Layer 0 Neuron 2625', 'Layer 0 Neuron 2670', 'Layer 0 Neuron 2773']\n",
        "Layer 1, Neurons: ['Layer 1 Neuron 11', 'Layer 1 Neuron 113', 'Layer 1 Neuron 376', 'Layer 1 Neuron 524', 'Layer 1 Neuron 672', 'Layer 1 Neuron 1685', 'Layer 1 Neuron 1766', 'Layer 1 Neuron 1838']\n",
        "Layer 2, Neurons: ['Layer 2 Neuron 57', 'Layer 2 Neuron 789', 'Layer 2 Neuron 923', 'Layer 2 Neuron 1316', 'Layer 2 Neuron 2719', 'Layer 2 Neuron 2801', 'Layer 2 Neuron 3062']\n",
        "Layer 3, Neurons: ['Layer 3 Neuron 323', 'Layer 3 Neuron 693', 'Layer 3 Neuron 1469', 'Layer 3 Neuron 1795', 'Layer 3 Neuron 2233']\n",
        "Layer 4, Neurons: ['Layer 4 Neuron 102', 'Layer 4 Neuron 170', 'Layer 4 Neuron 642', 'Layer 4 Neuron 915', 'Layer 4 Neuron 2116', 'Layer 4 Neuron 2548']\n",
        "Layer 5, Neurons: ['Layer 5 Neuron 55']\n",
        "Layer 6, Neurons: ['Layer 6 Neuron 1443', 'Layer 6 Neuron 1506', 'Layer 6 Neuron 1712']\n",
        "Layer 7, Neurons: []\n",
        "Layer 8, Neurons: ['Layer 8 Neuron 763']\n",
        "Layer 9, Neurons: ['Layer 9 Neuron 2528']\n",
        "Layer 10, Neurons: []\n",
        "Layer 11, Neurons: ['Layer 11 Neuron 1023']\n",
        "\n",
        "=================Input 3====================\n",
        "Layer 0, Neurons: ['Layer 0 Neuron 75', 'Layer 0 Neuron 283', 'Layer 0 Neuron 489', 'Layer 0 Neuron 2625', 'Layer 0 Neuron 2773']\n",
        "Layer 1, Neurons: ['Layer 1 Neuron 1685', 'Layer 1 Neuron 1766']\n",
        "Layer 2, Neurons: ['Layer 2 Neuron 789']\n",
        "Layer 3, Neurons: ['Layer 3 Neuron 693']\n",
        "Layer 4, Neurons: ['Layer 4 Neuron 170']\n",
        "Layer 5, Neurons: []\n",
        "Layer 6, Neurons: ['Layer 6 Neuron 1443']\n",
        "Layer 7, Neurons: []\n",
        "Layer 8, Neurons: ['Layer 8 Neuron 763']\n",
        "Layer 9, Neurons: ['Layer 9 Neuron 2528']\n",
        "Layer 10, Neurons: []\n",
        "Layer 11, Neurons: []\n",
        "\n",
        "=================Input 8====================\n",
        "Layer 0, Neurons: ['Layer 0 Neuron 75', 'Layer 0 Neuron 239', 'Layer 0 Neuron 283', 'Layer 0 Neuron 307', 'Layer 0 Neuron 489', 'Layer 0 Neuron 537', 'Layer 0 Neuron 765', 'Layer 0 Neuron 1669', 'Layer 0 Neuron 2018', 'Layer 0 Neuron 2625', 'Layer 0 Neuron 2670', 'Layer 0 Neuron 2773']\n",
        "Layer 1, Neurons: ['Layer 1 Neuron 11', 'Layer 1 Neuron 113', 'Layer 1 Neuron 376', 'Layer 1 Neuron 524', 'Layer 1 Neuron 672', 'Layer 1 Neuron 1685', 'Layer 1 Neuron 1766', 'Layer 1 Neuron 1838']\n",
        "Layer 2, Neurons: ['Layer 2 Neuron 57', 'Layer 2 Neuron 789', 'Layer 2 Neuron 923', 'Layer 2 Neuron 1316', 'Layer 2 Neuron 2719', 'Layer 2 Neuron 3062']\n",
        "Layer 3, Neurons: ['Layer 3 Neuron 323', 'Layer 3 Neuron 693', 'Layer 3 Neuron 1469', 'Layer 3 Neuron 1795', 'Layer 3 Neuron 2233']\n",
        "Layer 4, Neurons: ['Layer 4 Neuron 102', 'Layer 4 Neuron 170', 'Layer 4 Neuron 915']\n",
        "Layer 5, Neurons: ['Layer 5 Neuron 55']\n",
        "Layer 6, Neurons: ['Layer 6 Neuron 1506', 'Layer 6 Neuron 1712']\n",
        "Layer 7, Neurons: []\n",
        "Layer 8, Neurons: ['Layer 8 Neuron 763']\n",
        "Layer 9, Neurons: ['Layer 9 Neuron 2528']\n",
        "Layer 10, Neurons: []\n",
        "Layer 11, Neurons: ['Layer 11 Neuron 1023']\n",
        "\n",
        "=================Input 9====================\n",
        "Layer 0, Neurons: ['Layer 0 Neuron 75', 'Layer 0 Neuron 239', 'Layer 0 Neuron 283', 'Layer 0 Neuron 489', 'Layer 0 Neuron 2625', 'Layer 0 Neuron 2773']\n",
        "Layer 1, Neurons: ['Layer 1 Neuron 524', 'Layer 1 Neuron 1685', 'Layer 1 Neuron 1766']\n",
        "Layer 2, Neurons: ['Layer 2 Neuron 789']\n",
        "Layer 3, Neurons: ['Layer 3 Neuron 693']\n",
        "Layer 4, Neurons: ['Layer 4 Neuron 170']\n",
        "Layer 5, Neurons: []\n",
        "Layer 6, Neurons: ['Layer 6 Neuron 1443']\n",
        "Layer 7, Neurons: []\n",
        "Layer 8, Neurons: ['Layer 8 Neuron 763']\n",
        "Layer 9, Neurons: ['Layer 9 Neuron 2528']\n",
        "Layer 10, Neurons: []\n",
        "Layer 11, Neurons: []\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Mask after second layer of FFN\n",
        "preserved_neuron_list = [[75, 239, 283, 307, 489, 537, 2018, 2625, 2670, 2773, 765, 1669],\n",
        "                       [11, 113, 376, 524, 672, 1685, 1766, 1838],\n",
        "                       [57, 789, 923,1316 ,2719, 2801, 3062],\n",
        "                       [323, 693, 1469, 1795, 2233],\n",
        "                       [102, 170, 642, 915, 2116, 2548],\n",
        "                       [55],\n",
        "                       [1443, 1506, 1712],\n",
        "                       [],\n",
        "                       [763],\n",
        "                       [2528],\n",
        "                       [],\n",
        "                       [1023]]\n",
        "\n",
        "num_neurons = 3072\n",
        "masks = []\n",
        "\n",
        "original_list = list(range(3072))\n",
        "np.random.shuffle(original_list, )\n",
        "masked_neurons_list = [original_list[:2457]]*12\n",
        "\n",
        "for i, masked_neurons in enumerate(masked_neurons_list):\n",
        "    mask = np.ones(num_neurons)\n",
        "    if masked_neurons not in preserved_neuron_list[i]:\n",
        "      mask[masked_neurons] = 0\n",
        "      masks.append(mask)"
      ],
      "metadata": {
        "id": "-3BIP4Ww_NsY"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(masks))\n",
        "print(masks[0].shape)\n",
        "print(masks[0].reshape(-1,).shape)\n",
        "print(masks[0])"
      ],
      "metadata": {
        "id": "FYlH14e2DeZh",
        "outputId": "5df5ef86-dea1-4fd4-c389-a4680d2a6208",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12\n",
            "(3072,)\n",
            "(3072,)\n",
            "[0. 0. 0. ... 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer = 'tf_bert_model/bert/encoder/layer_._0/output/dense/kernel'\n",
        "layer.split('/')[3].split('_')"
      ],
      "metadata": {
        "id": "s4-y1ANaIirQ",
        "outputId": "5b97405c-a923-445e-cad5-51bcf760930d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['layer', '.', '0']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#zero out -> quantization + sparse matrix pruning\n",
        "# Assuming bert_model is your pre-trained BERT model\n",
        "#tf_bert_model/bert/encoder/layer_._0/output/dense/kernel\n",
        "\n",
        "for var in bert_model.variables:\n",
        "    if 'output/dense/kernel' in var.name and 'attention' not in var.name:\n",
        "        # Extract layer number from variable name\n",
        "        layer_num = int(var.name.split('/')[3].split('_')[2])\n",
        "\n",
        "        # Get the current weights\n",
        "        weights = var.numpy()\n",
        "\n",
        "        # Apply the mask #(3072,)\n",
        "        mask = masks[layer_num]\n",
        "        weights *= mask.reshape(-1, 1)  # Reshape mask and apply to weights\n",
        "\n",
        "        # Assign the modified weights back to the variable\n",
        "        var.assign(weights)\n"
      ],
      "metadata": {
        "id": "SCckoPE6G3IJ"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to check in if the weights are correctly zero out\n",
        "for var in bert_model.variables:\n",
        "    if 'output/dense/kernel' in var.name  and 'attention' not in var.name:  # Checking for the first layer as an example\n",
        "        print(var.name, var.numpy()[0:5, 0:5])  # Print a small section of the weights"
      ],
      "metadata": {
        "id": "BVmnG6doJy8A",
        "outputId": "05eaa69c-bf24-4968-879a-1d7ad8ac882a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf_bert_model/bert/encoder/layer_._0/output/dense/kernel:0 [[ 0. -0. -0.  0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [-0.  0. -0. -0.  0.]\n",
            " [-0. -0. -0.  0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]]\n",
            "tf_bert_model/bert/encoder/layer_._1/output/dense/kernel:0 [[-0.  0.  0. -0.  0.]\n",
            " [ 0.  0.  0. -0.  0.]\n",
            " [ 0. -0. -0. -0.  0.]\n",
            " [-0. -0. -0. -0. -0.]\n",
            " [ 0. -0. -0.  0.  0.]]\n",
            "tf_bert_model/bert/encoder/layer_._2/output/dense/kernel:0 [[-0.  0. -0.  0. -0.]\n",
            " [ 0.  0.  0. -0. -0.]\n",
            " [ 0. -0.  0. -0.  0.]\n",
            " [ 0.  0.  0. -0. -0.]\n",
            " [-0. -0. -0. -0. -0.]]\n",
            "tf_bert_model/bert/encoder/layer_._3/output/dense/kernel:0 [[ 0.  0.  0.  0. -0.]\n",
            " [-0. -0.  0. -0.  0.]\n",
            " [-0.  0.  0.  0.  0.]\n",
            " [-0. -0.  0.  0.  0.]\n",
            " [-0. -0. -0.  0. -0.]]\n",
            "tf_bert_model/bert/encoder/layer_._4/output/dense/kernel:0 [[-0. -0. -0.  0.  0.]\n",
            " [ 0. -0.  0.  0.  0.]\n",
            " [-0.  0. -0. -0. -0.]\n",
            " [-0.  0. -0.  0.  0.]\n",
            " [ 0. -0.  0.  0. -0.]]\n",
            "tf_bert_model/bert/encoder/layer_._5/output/dense/kernel:0 [[ 0. -0. -0. -0. -0.]\n",
            " [ 0. -0. -0. -0. -0.]\n",
            " [-0. -0.  0. -0. -0.]\n",
            " [-0. -0.  0.  0. -0.]\n",
            " [-0.  0.  0.  0. -0.]]\n",
            "tf_bert_model/bert/encoder/layer_._6/output/dense/kernel:0 [[ 0.  0.  0.  0. -0.]\n",
            " [ 0. -0.  0. -0. -0.]\n",
            " [ 0. -0.  0. -0.  0.]\n",
            " [ 0.  0. -0. -0. -0.]\n",
            " [-0.  0. -0.  0.  0.]]\n",
            "tf_bert_model/bert/encoder/layer_._7/output/dense/kernel:0 [[ 0. -0. -0. -0. -0.]\n",
            " [-0. -0. -0. -0. -0.]\n",
            " [ 0. -0.  0. -0.  0.]\n",
            " [-0. -0.  0. -0. -0.]\n",
            " [ 0. -0.  0. -0.  0.]]\n",
            "tf_bert_model/bert/encoder/layer_._8/output/dense/kernel:0 [[ 0.  0. -0. -0. -0.]\n",
            " [-0. -0. -0. -0.  0.]\n",
            " [-0. -0. -0. -0.  0.]\n",
            " [-0.  0.  0. -0. -0.]\n",
            " [ 0.  0. -0.  0.  0.]]\n",
            "tf_bert_model/bert/encoder/layer_._9/output/dense/kernel:0 [[-0. -0. -0. -0. -0.]\n",
            " [ 0. -0. -0. -0. -0.]\n",
            " [-0. -0.  0.  0.  0.]\n",
            " [-0. -0.  0.  0.  0.]\n",
            " [-0.  0. -0. -0.  0.]]\n",
            "tf_bert_model/bert/encoder/layer_._10/output/dense/kernel:0 [[-0.  0. -0. -0.  0.]\n",
            " [-0. -0. -0.  0. -0.]\n",
            " [-0. -0.  0.  0. -0.]\n",
            " [-0. -0. -0.  0. -0.]\n",
            " [-0.  0.  0.  0.  0.]]\n",
            "tf_bert_model/bert/encoder/layer_._11/output/dense/kernel:0 [[ 0. -0. -0. -0. -0.]\n",
            " [ 0.  0.  0. -0. -0.]\n",
            " [ 0. -0.  0.  0. -0.]\n",
            " [ 0.  0. -0. -0. -0.]\n",
            " [ 0.  0. -0. -0. -0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#bert_model has been zeroed out.\n",
        "bert_cls_model_classification.evaluate(bert_test_inputs, bert_test_labels)\n",
        "\n",
        "#accuarcy kept the same. after pruning 10% (307) of neurons (zeroing out). --0.843\n",
        "#test in sequence / directly pruning\n",
        "#50% (1531) -- 0.822\n",
        "#80% (2457) -- 0.747 / 0.71\n",
        "#90% (2764) -- 0.736 / not yet\n",
        "#95% (2922) -- 0.734 / 0.47\n",
        "\n",
        "#Impending for further verifying"
      ],
      "metadata": {
        "id": "SPvfNlaPV1_L",
        "outputId": "557d242d-ab63-47f0-e08b-f82b4a4193ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 7s 214ms/step - loss: 0.5675 - accuracy: 0.7190\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5675219893455505, 0.718999981880188]"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #load the model\n",
        "# with custom_object_scope({'TFBertModel': TFBertModel}):\n",
        "#     bert_cls_model_classification = tf.keras.models.load_model(model_h5_path)\n",
        "# Can't load the model and pruning it. it's different."
      ],
      "metadata": {
        "id": "c4PYZPp2b9ax"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameter Quantization is easier. low cosine similarity -> lower the digit from 32 to 2.\n",
        "#1. Directly zero out the FFN-> quantization + pruning\n",
        "#2. Need to check it out if we assign 0. I think tensorflow would ask us to keep the consistency of matrix to float32. so here, zeroed out item is still saved as float32 and not benifits from zeroing out\n",
        "#3. Need to check if the GPU can benefit  from sparse input. some GPU or powered by well-designed SW can speed up computation of sparse matrix.\n",
        "#4. Attension layer?"
      ],
      "metadata": {
        "id": "t1TEiAz0THXV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}