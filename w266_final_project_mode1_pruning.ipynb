{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+gWqhSctZlV6oa/kHKVAG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyanChen12035/w266-NLP/blob/main/w266_final_project_mode1_pruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaFKyXspi5kp"
      },
      "outputs": [],
      "source": [
        "!pip install pydot --quiet\n",
        "!pip install tensorflow-datasets --quiet\n",
        "!pip install transformers --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "id": "pgXwMWO2i6dO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "metadata": {
        "id": "UzfrZYRMi7rY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Embedding, Input, Dense, Lambda, Dropout, Conv1D, GlobalMaxPooling1D, Concatenate, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow_datasets as tfds\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from transformers import logging\n",
        "logging.set_verbosity_error()\n",
        "import sklearn as sk\n",
        "import os\n",
        "from nltk.data import find\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from tensorflow.keras.utils import custom_object_scope"
      ],
      "metadata": {
        "id": "yPa__T8ci8ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = tfds.load(\n",
        "    name=\"imdb_reviews\",\n",
        "    split=('train[:80%]', 'test[80%:]'),\n",
        "    as_supervised=True)\n",
        "\n",
        "train_examples, train_labels = next(iter(train_data.batch(20000)))\n",
        "test_examples, test_labels = next(iter(test_data.batch(5000)))"
      ],
      "metadata": {
        "id": "MQRwueBNi9dY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#allow us to get the hidden layer\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-cased', output_hidden_states=True)\n",
        "MAX_SEQUENCE_LENGTH = 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "bGsFfdYZjBal",
        "outputId": "4a415ee0-1bf4-49d0-88a0-c6469d09b14c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bdcd8253ef20>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#allow us to get the hidden layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbert_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-cased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbert_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFBertModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-cased'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BertTokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#BERT Tokenization of training and test data\n",
        "#Embedding size of Bert tokenizer: 768\n",
        "#Dictionary size of Bert tokenizer: 28,996\n",
        "\n",
        "\n",
        "train_examples_str = [x.decode('utf-8') for x in train_examples.numpy()]\n",
        "test_examples_str = [x.decode('utf-8') for x in test_examples.numpy()]\n",
        "\n",
        "bert_train_tokenized = bert_tokenizer(train_examples_str,\n",
        "              max_length=MAX_SEQUENCE_LENGTH,\n",
        "              truncation=True,\n",
        "              padding='max_length',\n",
        "              return_tensors='tf')\n",
        "bert_train_inputs = [bert_train_tokenized.input_ids,\n",
        "                     bert_train_tokenized.token_type_ids,\n",
        "                     bert_train_tokenized.attention_mask]\n",
        "bert_train_labels = np.array(train_labels)\n",
        "\n",
        "bert_test_tokenized = bert_tokenizer(test_examples_str,\n",
        "              max_length=MAX_SEQUENCE_LENGTH,\n",
        "              truncation=True,\n",
        "              padding='max_length',\n",
        "              return_tensors='tf')\n",
        "bert_test_inputs = [bert_test_tokenized.input_ids,\n",
        "                     bert_test_tokenized.token_type_ids,\n",
        "                     bert_test_tokenized.attention_mask]\n",
        "bert_test_labels = np.array(test_labels)"
      ],
      "metadata": {
        "id": "YWI5pXdqjF5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#12 layers of transformer\n",
        "#A drop out layer + dense layer with 100 hidden layer size on top + final layer with sigmoid as activation function\n",
        "\n",
        "def create_bert_cls_model(bert_base_model,\n",
        "                          max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "                          hidden_size = 100,\n",
        "                          dropout=0.3,\n",
        "                          learning_rate=0.00005,\n",
        "                          output_cls_tokens=False):\n",
        "    \"\"\"\n",
        "    Build a simple classification model with BERT. Use the CLS Token output for classification purposes.\n",
        "    \"\"\"\n",
        "\n",
        "    bert_base_model.trainable = True #True\n",
        "\n",
        "    #input layers of BERT, shape (batch, max_sequence_length), model will be fit with bert_train_tokenized\n",
        "    input_ids = Input(shape=(max_sequence_length,), dtype=tf.int32, name='input_ids')\n",
        "    token_type_ids = Input(shape=(max_sequence_length,), dtype=tf.int32, name='token_type_ids')\n",
        "    attention_mask = Input(shape=(max_sequence_length,), dtype=tf.int32, name='attention_mask')\n",
        "\n",
        "    inputs = [input_ids, token_type_ids, attention_mask]\n",
        "\n",
        "    #BERT output, last_hidden_state shape (batch, max_sequence_length, embedding dimensions)\n",
        "    bert_output = bert_base_model(input_ids=input_ids,\n",
        "                                  token_type_ids=token_type_ids,\n",
        "                                  attention_mask=attention_mask,\n",
        "                                  output_hidden_states=output_cls_tokens)\n",
        "\n",
        "    #Extract the CLS token's output, the embedding representation of first token of every sentence, shape(batch, embedding dimensions)\n",
        "    cls_token_output = bert_output[0][:, 0, :] # CLS token output from the last layer\n",
        "\n",
        "    #Add a dropout layer\n",
        "    x = Dropout(dropout)(cls_token_output)\n",
        "\n",
        "    #Add a fully connected layer for classification\n",
        "    x = Dense(hidden_size, activation='relu')(x)\n",
        "\n",
        "    #Final output layer for classification, assuming it's binary task\n",
        "    output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "\n",
        "    # CLS output for each layer of transformer\n",
        "    if output_cls_tokens:\n",
        "        cls_outputs = [state[:, 0, :] for state in bert_output[2]] # CLS token outputs from all layers\n",
        "        model_outputs = [output] + cls_outputs\n",
        "\n",
        "    else:\n",
        "        model_outputs = output\n",
        "\n",
        "\n",
        "    #Model complie\n",
        "    classification_model = Model(inputs=inputs, outputs=model_outputs)\n",
        "    classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                                 loss='binary_crossentropy',\n",
        "                                 metrics=['accuracy'])\n",
        "\n",
        "    return classification_model\n",
        "\n",
        "\"\"\"\n",
        "bert_output[2]: When the output_hidden_states parameter is set to True, this output provides the hidden states from all layers of the BERT model.\n",
        "It is a list of tensors, where each tensor corresponds to the hidden states of a specific layer.\n",
        "The shape of each tensor in this list is (batch_size, sequence_length, hidden_size), similar to bert_output[0], but for each individual layer.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "mMYL0NuDjHVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "y9xtOOZvjI_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bert_model\n",
        "bert_cls_model_classification = create_bert_cls_model(bert_model, output_cls_tokens=False)\n",
        "history_cls_bert= bert_cls_model_classification.fit(bert_train_inputs,\n",
        "                                                    bert_train_labels,\n",
        "                                                    epochs=2, #2\n",
        "                                                    batch_size=8, #8\n",
        "                                                    validation_data=(bert_test_inputs, bert_test_labels))"
      ],
      "metadata": {
        "id": "CFoto5jDjKCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save the model\n",
        "# Assuming 'bert_cls_model_classification' is your trained model\n",
        "model_h5_path = \"content/sample_data/save/model_finetuned_BERT.h5\"  # Replace with your desired path\n",
        "\n",
        "# Register TFBertModel as a custom object\n",
        "with custom_object_scope({'TFBertModel': TFBertModel}):\n",
        "    bert_cls_model_classification.save(model_h5_path)"
      ],
      "metadata": {
        "id": "y2kLS7etjKao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load the model\n",
        "with custom_object_scope({'TFBertModel': TFBertModel}):\n",
        "    bert_cls_model_classification_ = tf.keras.models.load_model(model_h5_path)"
      ],
      "metadata": {
        "id": "HrwgJoWfjMQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Save and load the model\n",
        "#2. Trim off FFN (using low rank approximation instead?) 要怎麼砍?? 直接砍會被限制shape而且感覺不聰明, 用low rank approximation? 或者是PCA 之類的? Quantization?\n",
        "# Parameter Quantization is easier. low cosine similarity -> lower the digit from 32 to 2.\n",
        "\n",
        "#split into two code book. research and trimming\n",
        "#read the paper of quantization + low rank approximation\n",
        "#low rank approximation\n",
        "#3. Attension layer\n",
        "#4. Trim off attention\n",
        "#5. split the data into training, valdition and testing\n",
        "#5. evaluation\n",
        "# retrain it, the model we get is different in every training. save it as another file"
      ],
      "metadata": {
        "id": "8W9NX3RxjQWQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}