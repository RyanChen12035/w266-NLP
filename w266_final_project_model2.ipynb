{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/j14saxiQFR7CFBq/GchR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyanChen12035/w266-NLP/blob/main/w266_final_project_model2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FCVzLM1vf22"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "1. Use GPT2-medium, Decoder based, pretrained with EN text and has 24 layers of transformer layers. On top of it is a projection layer with shape(768, 50257) and softmax layer\n",
        "2. It was pretrained by EN text, so basically it can not handle EN to CN task without fine-tuning\n",
        "3. Need a token that incorperate EN and CN, the embedding size and dictionary size are (768, 119,547)\n",
        "4. Fine-tune, may need prompt + EN,CN pair. Detailed training process needs to be further clearfy.\n",
        "5. The FFN consists of two linear transformations with weights of shapes (768, 3072) and (3072, 768)\n",
        "6. Only need a look-ahead mask before the first layer of transformer. For following layers, input would be pass down one by one without seeing future information.\n",
        "7. No CLS token in decoder based model, so after 10 runs of decoding process (assuming we have 10 input length, pass one token and previous input for\n",
        "   next layer each time), we would have (batch, sequence, embedding dimensions) as the shape of each transformer layer. We can take mean pooling or max pooling\n",
        "   to have the representation of each layers of transformer.\n",
        "8. The shape of representation would be (1,768) and we can put it into the comparison of neurons in FFN.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from pprint import pprint\n",
        "from IPython.display import clear_output\n",
        "from transformers import BertTokenizer, GPT2LMHeadModel\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Embedding, Input, Dense, Lambda, Dropout, Conv1D, GlobalMaxPooling1D, Concatenate, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow_datasets as tfds\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from transformers import logging\n",
        "logging.set_verbosity_error()\n",
        "import sklearn as sk\n",
        "import os\n",
        "from nltk.data import find\n",
        "import matplotlib.pyplot as plt\n",
        "import re"
      ],
      "metadata": {
        "id": "gnEPxQUT7maL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "id": "I1Mn3wO18Pyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "metadata": {
        "id": "XO8n8Dve8QKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check out the data source we have\n",
        "tmp_builder = tfds.builder(\"wmt19_translate/zh-en\")\n",
        "pprint(tmp_builder.subsets)"
      ],
      "metadata": {
        "id": "4o_uCgDC7o-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#download data by tfds.builder\n",
        "config = tfds.translate.wmt.WmtConfig(\n",
        "  version=tfds.core.Version('0.0.3'),\n",
        "  language_pair=(\"zh\", \"en\"),\n",
        "  subsets={\n",
        "    tfds.Split.TRAIN: [\"newscommentary_v14\"]\n",
        "  }\n",
        ")\n",
        "builder = tfds.builder(\"wmt_translate\", config=config)\n",
        "builder.download_and_prepare(download_dir=download_dir)\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "_QLh0ijf7pDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set builder to dataset(data pipeline type), split it into training, validation, testing\n",
        "examples = builder.as_dataset(split=['train[:20%]','train[20%:21%]','train[21%:]'], as_supervised=True)"
      ],
      "metadata": {
        "id": "DPeb3nRe7pMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#leave the testing examples this time.\n",
        "train_examples, val_examples, _ = examples\n",
        "print(train_examples)\n",
        "print(val_examples)"
      ],
      "metadata": {
        "id": "gkbO6ezJ78e1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_examples = []\n",
        "num_samples = 10\n",
        "\n",
        "for en_t, zh_t in train_examples.take(num_samples):\n",
        "  en = en_t.numpy().decode(\"utf-8\")\n",
        "  zh = zh_t.numpy().decode(\"utf-8\")\n",
        "\n",
        "  print(en)\n",
        "  print(zh)\n",
        "  print('-' * 10)\n",
        "\n",
        "\n",
        "  sample_examples.append((en, zh))"
      ],
      "metadata": {
        "id": "Fbx2qrZJ78hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize a tokenizer for English and Chinese\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "# Initialize the GPT-2 model\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")"
      ],
      "metadata": {
        "id": "F_eoyYwOv2us"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}