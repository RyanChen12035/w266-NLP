{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqdTypiKjOCQ"
      },
      "source": [
        "# Assignment 3: Question Answering with a Language Model\n",
        "\n",
        "**Description:** This assignment covers question answering with a language model. There are many ways to formulate the question answering task and this is one of them.  You will use the masked token with T5 to develop a sentence construct that allows the model to answer the question more than 75% of the time. You should also be able to develop an intuition for:\n",
        "\n",
        "\n",
        "* Working with masked language models\n",
        "* Working with prompt based models\n",
        "* The depths and limits of knowledge in these large models\n",
        "\n",
        "\n",
        "This notebook does NOT require a GPU to work in a timely fashion. This notebook should be run on a Google Colab even though it does not require a GPU. By default, when you open the notebook in Colab it will not configure a GPU.\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/datasci-w266/2023-fall-main/blob/master/assignment/a3/QuestionAnswering_test.ipynb)\n",
        "\n",
        "\n",
        "**INSTRUCTIONS:**\n",
        "\n",
        "* Questions are always indicated as **QUESTION:**, so you can search for this string to make sure you answered all of the questions. You are expected to fill out, run, and submit this notebook, as well as to answer the questions in the **answers** file as you did in a1 and a2.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14QhwWvujXh-"
      },
      "outputs": [],
      "source": [
        "!pip install -q sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hldMurd9pnTl"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phetFLjypnc9"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBWFf_-1pnxK"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, TFT5ForConditionalGeneration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9-jQ_zezZg7"
      },
      "outputs": [],
      "source": [
        "t5_model = TFT5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "\n",
        "t5_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRvBDemGBBAp"
      },
      "source": [
        "\"\\<extra_id_0\\>\" is the special token we can use with T5 to invoke its masked word modeling ability.  This means we can construct sentences, like a fill in the blank test, that allow us to probe the knowledge embedded in the model based on its pre-training.  Here's an example that works well.  We can construct with the special token a prompt sentence that says \"A poodle is a type of \"\\<extra_id_0\\>\"\".  We expect the model to fill in the word 'dog' as it predicts the missing word.  Note that it may also predict 'pet' as another possibility as a poodle can be a type of pet.  Remember the\n",
        "\"\\<extra_id_0\\>\" token can appear anywhere in the sentence, not just at the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nt6yMNk5pnzG"
      },
      "outputs": [],
      "source": [
        "PROMPT_SENTENCE = ( \"A poodle is a type of <extra_id_0> .\")\n",
        "t5_input_text = PROMPT_SENTENCE\n",
        "t5_inputs = t5_tokenizer([t5_input_text], return_tensors='tf')\n",
        "t5_summary_ids = t5_model.generate(t5_inputs['input_ids'],\n",
        "                                   num_beams=9,\n",
        "                                   no_repeat_ngram_size=1,\n",
        "                                   num_return_sequences=3,\n",
        "                                   min_length=1,\n",
        "                                   max_length=5)\n",
        "\n",
        "print([t5_tokenizer.decode(g, skip_special_tokens=True,\n",
        "                           clean_up_tokenization_spaces=False) for g in t5_summary_ids])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjEDY_mSTpqR"
      },
      "source": [
        "After you've run it once, try substituting 'beagle' for 'poodle' and you'll see the model gets confused.\n",
        "\n",
        "Notice too that we are using a beam search approach to generate multiple possibilities but only accept the top three choices rather than just the first choice. We're asking for three answer sequences to be returned and they should be between 1 and 5 subwords long.\n",
        "\n",
        "With the growth of text generation models, developing a good prompt is an increasingly important skill.\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        "1.1 Let's test the actual knowledge encoded in the T5 model. Let's construct prompts that return provably true or false facts like you might see on a fill in the blank test.  Given the following ten countries (England, France, Germany, Russia, Egypt, Thailand, Japan, Canada, India, China) construct **two** different PROMPT_SENTENCEs using the special token and the values of the countries list so that in at least 7 of the 10 cases one of the top three answers is a provably correct fact.  Use the string COUNTRY to stand in for each of the elements in the list.  For example, \"\\<extra_id_0\\> is the chief export of COUNTRY\".\n",
        "\n",
        "Note that a fact usually takes the form of a noun phrase - verb phrase - noun phrase triple where one of those noun phrases will consist of the country value.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAKwWNoCUi6c"
      },
      "outputs": [],
      "source": [
        "#Use this space to craft your first sentence.  You do NOT need to modify the hyperparameters!\n",
        "PROMPT_SENTENCE = ( \"Fill in the <extra_id_0> is a form of question.\")\n",
        "t5_input_text = PROMPT_SENTENCE\n",
        "t5_inputs = t5_tokenizer([t5_input_text], return_tensors='tf')\n",
        "t5_summary_ids = t5_model.generate(t5_inputs['input_ids'],\n",
        "                                   num_beams=9,\n",
        "                                   no_repeat_ngram_size=2,\n",
        "                                   num_return_sequences=3,\n",
        "                                   min_length=1,\n",
        "                                   max_length=3)\n",
        "\n",
        "print([t5_tokenizer.decode(g, skip_special_tokens=True,\n",
        "                           clean_up_tokenization_spaces=False) for g in t5_summary_ids])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XenG0Rr0l3z0"
      },
      "outputs": [],
      "source": [
        "#Use this space to craft your second sentence.  You do NOT need to modify the hyperparameters!\n",
        "PROMPT_SENTENCE2 = ( \"<extra_id_0> is another form of test question.\")\n",
        "t5_input_text = PROMPT_SENTENCE2\n",
        "t5_inputs = t5_tokenizer([t5_input_text], return_tensors='tf')\n",
        "t5_summary_ids = t5_model.generate(t5_inputs['input_ids'],\n",
        "                                   num_beams=9,\n",
        "                                   no_repeat_ngram_size=2,\n",
        "                                   num_return_sequences=3,\n",
        "                                   min_length=1,\n",
        "                                   max_length=3)\n",
        "\n",
        "print([t5_tokenizer.decode(g, skip_special_tokens=True,\n",
        "                           clean_up_tokenization_spaces=False) for g in t5_summary_ids])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}